{
    "abstract": "We describe, analyze, and experiment with a framework for empirical\nloss minimization with regularization. Our algorithmic framework\nalternates between two phases. On each iteration we first perform an\n<i>unconstrained</i> gradient descent step. We then cast and solve an\ninstantaneous optimization problem that trades off minimization of a\nregularization term while keeping close proximity to the result of\nthe first phase. This view yields a simple yet effective algorithm\nthat can be used for batch penalized risk minimization and online\nlearning. Furthermore, the two phase approach enables sparse\nsolutions when used in conjunction with regularization functions\nthat promote sparsity, such as <i>l</i><sub>1</sub>. We derive concrete and very\nsimple algorithms for minimization of loss functions with <i>l</i><sub>1</sub>,\n<i>l</i><sub>2</sub>, <i>l</i><sub>2</sub><sup>2</sup>, and <i>l</i><sub>&infin;</sub> regularization. We also show\nhow to construct efficient algorithms for mixed-norm <i>l</i><sub>1</sub>/<i>l</i><sub>q</sub>\nregularization. We further extend the algorithms and give efficient\nimplementations for very high-dimensional data with sparsity. We\ndemonstrate the potential of the proposed framework in a series of\nexperiments with synthetic and natural data sets.",
    "authors": [
        "John Duchi",
        "Yoram Singer"
    ],
    "id": "duchi09a",
    "issue": 98,
    "pages": [
        2899,
        2934
    ],
    "title": "Efficient Online and Batch Learning Using Forward Backward Splitting",
    "volume": "10",
    "year": "2009"
}