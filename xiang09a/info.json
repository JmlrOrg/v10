{
    "abstract": "This paper considers binary classification algorithms generated\nfrom Tikhonov regularization schemes associated with general\nconvex loss functions and varying Gaussian kernels. Our main goal\nis to provide fast convergence rates for the excess\nmisclassification error. Allowing varying Gaussian kernels in the\nalgorithms improves learning rates measured by regularization\nerror and sample error. Special structures of Gaussian kernels\nenable us to construct, by a nice approximation scheme with a\nFourier analysis technique, uniformly bounded regularizing\nfunctions achieving polynomial decays of the regularization error\nunder a Sobolev smoothness condition. The sample error is\nestimated by using a projection operator and a tight bound for the\ncovering numbers of reproducing kernel Hilbert spaces generated by\nGaussian kernels. The convexity of the general loss function plays\na very important role in our analysis.",
    "authors": [
        "Dao-Hong Xiang",
        "Ding-Xuan Zhou"
    ],
    "id": "xiang09a",
    "issue": 48,
    "pages": [
        1447,
        1468
    ],
    "title": "Classification with Gaussians and Convex Loss",
    "volume": "10",
    "year": "2009"
}